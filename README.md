# ğŸ² Recipe Summarizer

âš¡ Objective: 
Given a food image and a noisy or vague dish title, the model generates a concise 2â€“3 step summary of the cooking instructions that captures the essence of the dish.

ğŸŒ Overview:
Use of the Llava v1.6 Mistral-7b vision-language model (VLM) to generate concise 2â€“3 step cooking summaries from detailed recipe instructions, supported by an image and a vague or noisy title.
The system implements a few-shot prompting strategy by feeding the model:
â€¢	Several annotated examples (training_data)
â€¢	A new test recipe (test_data) and asking it to summarize the essential steps in a short, clear format.

ğŸ’¡ Pipeline Abstract:
Input:
â€¢	Recipe title (often vague, e.g., â€œround meat thingsâ€ for meatballs)
â€¢	Full detailed cooking instructions and manual summary.
â€¢	Corresponding image.
â€¢	The dataset was collected by web scraping recipes from the AllRecipes website, gathering detailed cooking instructions, actual titles, and accompanying food images.
Model:
â€¢	llava-hf/llava-v1.6-mistral-7b-hf
A multimodal large language model capable of visual and text reasoning.
Prompting Approach:
â€¢	Uses few-shot examples embedded in a system prompt to guide the summarization style.
Output:
â€¢	A clear, concise, 2â€“3 step cooking summary.


ğŸ’¬ Approach:
Model: Llava v1.6 Mistral-7b:
â€¢	Llava is explicitly designed for joint vision and language reasoning. It can process both the image and text together, allowing it to disambiguate vague titles (like â€œround meat thingsâ€) by grounding them in the visual context. This is something text only models or weaker VLMs struggle with.
â€¢	At 7 billion parameters, the Mistral-based Llava is smaller and faster than giant models but still delivers high-quality multimodal outputs. This makes it computationally efficient for testing and, avoiding the cost and slowness of heavier alternatives.
â€¢	Compared to earlier models (including BLIP, which I initially tested), Llava showed a clearer grasp of prompt patterns and could reliably follow a summarization style when given proper prompting. BLIP, while decent for image captioning or retrieval, often produced generic or incomplete summaries when asked to condense full recipes it lacked the refined instruction-following and multitask capabilities that Llava offers.
â€¢	Llavaâ€™s open-source nature gave flexibility and transparency, without needing fine-tuning. It allowed a plug and play setup using few-shot prompting, steering outputs by example rather than retraining.
Few-shot Prompting:
â€¢	Few-shot prompting needs zero additional training, just carefully crafted prompts with inlined examples.
â€¢	The summarization task has a specific target: short, action-focused, clear steps. By providing annotated examples in the prompt, I could guide the model to mimic the desired output format (concise 2â€“3 step summaries) without any weight updates.
â€¢	Few-shot setups let me swap in new examples or adjust the prompt on the fly, testing how the model adapts to different types of recipes (soups, drinks, baked goods) without waiting for re-training cycles.
â€¢	I could experiment across models (including BLIP and Llava) using similar prompt frameworks. This ultimately revealed Llavaâ€™s superior performance, especially when BLIPâ€™s outputs remained shallow, vague, or inconsistent even under carefully curated prompts.


ğŸ“Š Analysis of Performance:
âœ… Rights:
â€¢	For recipes like grilled chicken or smoothie, the model accurately condensed the core preparation steps (marinate + grill, blend + adjust).
â€¢	With the given visual context, Llava could correctly map vague titles like â€œround meat thingsâ€ to meatballs, using the image to ground its reasoning.
â€¢	The generated summaries usually maintained clear, action-ready steps rather than abstract descriptions or verbose answers..
âŒ Wrongs:
â€¢	In some cases, the model compressed too aggressively, omitting key sub-steps like preheating ovens or letting dishes rest.
â€¢	And in some cases where the tile was very vague like â€œcreamy riceâ€  the model relied more on the full instructions and comparably lengthy summaries were observed.
â€¢	For less vague titles like â€œwarm bowl stuffâ€, visual cues were underused.

ğŸ“ Project Structure
/images/                      â†’ Folder containing all recipe and food images
README.md                     â†’ Project overview and documentation
VLM_Cooking_Summary_v13.ipynb â†’ Main Jupyter Notebook with code, experiments, and results
dataset.json                  â†’ Collected recipe dataset (web scraped) in JSON format
recipe_summary_results.json   â†’ Model-generated cooking summaries (output results)
requirements.txt              â†’ Python dependencies list for setting up the environment


